# -*- coding: utf-8 -*-
"""Spam_SMS_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xQVz0ADgb89XpjZkrdYiFOJK3r2o7_VV
"""

import pandas as pd

# Load the dataset
data = pd.read_excel('/content/spam.xls')
# Display the first few rows of the dataset
print(data.head())

# Get a summary of the dataset
print(data.info())

# Check for missing values
print(data.isnull().sum())

# Print the column names to understand the structure
print(data.columns)

# Plot the distribution of labels
import matplotlib.pyplot as plt

# Drop unnecessary columns
data = data[['v1', 'v2']]

# Rename columns for better understanding
data.columns = ['Label', 'Message']

# Encode the labels (spam=1, ham=0)
data['Label'] = data['Label'].map({'spam': 1, 'ham': 0})

# Display the distribution of labels
print(data['Label'].value_counts())


plt.figure(figsize=(6, 6))
data['Label'].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, colors=['red', 'green'], labels=['Spam', 'Ham'])
plt.title('Distribution of Spam and Ham Messages')
plt.ylabel('')
plt.show()

from sklearn.model_selection import train_test_split

X = data['Message']
y = data['Label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.feature_extraction.text import TfidfVectorizer

# Vectorize the text data
vectorizer = TfidfVectorizer(stop_words='english', max_features=50000)
X_train_vect = vectorizer.fit_transform(X_train.astype(str).tolist())
X_test_vect = vectorizer.transform(X_test.astype(str).tolist())
X_train.shape

from sklearn.naive_bayes import MultinomialNB

# Initialize the Naive Bayes classifier
nb = MultinomialNB()

# Train the model
nb.fit(X_train_vect, y_train)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns

# Predict on the test data
y_pred = nb.predict(X_test_vect)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))

# Plot confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

import numpy as np

# Get feature names and their corresponding importance (log-probability of being spam)
feature_names = vectorizer.get_feature_names_out()
spam_prob = nb.feature_log_prob_[1]
ham_prob = nb.feature_log_prob_[0]

# Calculate the difference in log-probabilities
feature_importance = spam_prob - ham_prob

# Get the top 20 features
top_n = 20
top_features = np.argsort(feature_importance)[-top_n:]

# Plot the top features
plt.figure(figsize=(10, 6))
plt.barh(range(top_n), feature_importance[top_features], align='center')
plt.yticks(range(top_n), [feature_names[i] for i in top_features])
plt.xlabel('Importance')
plt.title('Top 20 Features for Spam Detection')
plt.show()